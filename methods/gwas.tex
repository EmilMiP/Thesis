%This section will briefly go over what a genome-wide association study (GWAS) is, some common considerations and models. A GWAS is usually performed on a single SNP at a time, rather than all SNPs at the same time. This is due to the computational cost of analysing data sets of the sizes that are usually present in biobanks and due to there being more SNPs than individuals. There are several potential models that can be used to analyse genotypes. One method is the Cochran-Armitage test \cite{cochran1954some,armitageTest}, which tests for independence in a $ 2\times 3 $ contingency table. However, this test is not able to incorporate covariates to account for, e.g. population stratification. A regression based method is usually preferred, as it allows for covariates to be included. One downside of using regression models is the assumption that the SNP effects will be additive, which is not the case in the Cochran-Armitage test. The genetic data for regression is usually coded as $ AA = 0 $, $ Aa = 1 $, and $ aa = 2 $, where $ A $ is the major allele and $ a $ is the minor allele\cite{zeng2015statistical}. When restricting to only additive genetic effects, there is no difference between logistic or linear regression and the Cochran-Armitage test. In short, the regression methods are preferred over the Cochran-Armitage test as covariates can be included and linear regression is preferred over logistic regression, since it is more computationally efficient and there is no difference between their power\cite{sikorska2013gwas,prive2019making,balding2006tutorial}.



This section will briefly go over what a genome-wide association study (GWAS) is, some common considerations, and models used. It will be split into sections that each cover an important topic for performing a GWAS, namely controlling type 1 errors, computational efficiency, and power improvement. At the end, we will also provide a non-exhaustive list of methodological advancements that excel in one or more of these topics. 

A GWAS is usually performed on a single SNP at a time, rather than all SNPs at the same time, meaning effect sizes are marginal instead of simultaneous. There are several potential models that can be used to analyse genotypes, and in the early days of GWAS the Cochran-Armitage test \cite{cochran1954some,armitageTest} was used \cite{balding2006tutorial}. The Cochran-Armitage test tests for independence in a $ 2\times 3 $ contingency table. However, this test is not able to incorporate covariates to account for important covariates such as population stratification. Therefore, regression based methods become popular, as they allow for covariates to be included. If a GWAS is performed with a regression, it implicitly assumed that the genetic effect from a given SNP will be additive, which is not the case for a Cochran-Armitage test. The implicit assumption follows from how the genetic data is coded for regression as $ AA = 0 $, $ Aa = 1 $, and $ aa = 2 $, where $ A $ is the major allele and $ a $ is the minor allele\cite{zeng2015statistical}. When restricting to only additive genetic effects, there is no difference between logistic or linear regression and the Cochran-Armitage test\textbf{TODO:REF}. 

In short, regression methods are preferred over the Cochran-Armitage test as covariates can be included and linear regression is preferred over logistic regression, since it is more computationally efficient and there is no difference between their power\cite{sikorska2013gwas,prive2019making,balding2006tutorial}.

First, the simplest and most common way to perform a GWAS will be introduced. Then each of the three important topics will be described and solutions to the problems will be presented.

\subsection{Linear regression GWAS}
The simplest and most computationally efficient way to test association between a SNP and an outcome, even when the outcome is binary, is with linear regression. If we have $ N $ individuals where we observe a set of $ M $ SNPs, then a linear regression GWAS of a single SNP can be described in the following way.

Let $ y $ denote the $ N\times1 $ vector of phenotypes for each individual, either binary or quantitative, $ X $ be the $ N \times (k+1) $ matrix containing $ k $ covariates and the intercept, $ G_j $ is a $ N\times 1 $ vector containing the $ j^{th} $ SNP, then the model is given by:

\begin{equation}\label{eq:baseGWAS}
y = \beta G_{j} +  X\gamma + \varepsilon.
\end{equation}
Where $ \beta $ denotes the genetic effect size, $ \gamma $ denotes a $ (k + 1) \times 1$ vector of coefficients for the intercept and covariates, $ \varepsilon $ is a $ N \times 1 $ vector of independent normally distributed noise. Going forward, we will assume that both $ y $ and $ G_j $ are scaled to have mean $ 0 $ and variance $ 1 $. The hypothesis being tested is $ H_0: \beta = 0 $ against $ H_A: \beta \neq 0 $. One of the most common ways to perform the test is with a Wald test $ Z = \hat{\beta}/\text{se}(\hat{\beta}) \sim N(0,1)$. 



\subsubsection{Linear mixed model GWAS}
A linear mixed model is an extension of a linear regression model. The linear mixed model adds a random effect to the model given in \cref{eq:baseGWAS}. With all other parameters being the same, we get


\begin{equation}\label{eq:baseMixedModelGWAS}
y = \beta G_{j} +  X\gamma + u + \varepsilon \qquad u \sim N(\mathbf{0}, \Sigma)
\end{equation}
The random term $ u $ and the noise $ \varepsilon $ are independent. Here $ \Sigma $ is a covariance structure that describes the observations $ y $. In a GWAS setting, the covariance structure is often estimated by the genetic relatedness matrix, which measures the correlation between all pairs of genotypes. Computationally, linear mixed models are far more intensive than linear regression, but the benefit of these models is their ability to boost power over simple linear regression.


\subsection{Controlling type-1 errors}
A common cause of type-1 errors is population structure. It is a term that covers several types of potential biases in a GWAS. These biases can result in spurious associations between SNPs and phenotypes, when there is no true association. The most common reasons for population structure in genotype data is due to \textit{population stratification} and \textit{related individuals}. Population stratification can have many causes. Every population will have some local structure, which may be problematic if not accounted for\textbf{REF?}, but having two or more \textit{genetic ancestries} in the data in particular is important to highlight. Genetic ancestries could severely bias a GWAS\textbf{REF?} and it is easy to account for. Regardless of the source of bias, they all result in the same underlying problem, namely artificial differences or similarities between a case and control group, which either creates a spurious association or masks a true association. \textbf{RFEFERENCE TO POP STRAT PROBLEMS?}
For example, if there is some kind of population structure in the data, common reasons are genetic ancestry or local variations within a population. A spurious association may occur if a subpopulation is particularly enriched with one type of variant and the rest of the population is not. Similarly, if the effect of a SNP in one subpopulation increases the risk, while it decreases the risk in a different subpopulation, then the effect of the given SNP would be hidden to us.

\subsubsection{Population stratification}
Within a population of individuals, it has been shown that there can be subpopulations where allele frequencies differ between subpopulations\cite{abdellaoui2013association,genome2014whole}. As mentioned above, it can cause artificial differences or similarities between the subpopulations when performing associations tests. One example of a spurious association driven by population stratification is the chopstick gene, which allegedly accounted for half of the variance in being able to eat with chopsticks \cite{marees2018tutorial,hamer2000beware}. A common and simple solution to account for local population stratification is by performing a PCA on the genotypes and including the first PCs as covariates in the association analysis \cite{price2006principal,price2010new,prive2020efficient}.

\textbf{SHOULD I GO OVER HOW A PCA IS PERFORMED IN GENOTYPE DATA? PCA IS MENTIONED HERE AND IN ANCESTRY}

Population stratification can have many sources, and the above solution works if only local subpopulations are present in an otherwise homogeneous population. A problem arises if there are two or more genetic ancestries, as the PCs will not be able to properly account for such stratification. As a results, it seems prudent to highlight this particular cause of population stratification. Analysing different ancestries together in a GWAS is not commonly done. This is due to different ancestries having different minor allele frequencies for certain SNPs, altogether different variants on certain positions, etc.\cite{helgason2005icelandic}. Therefore, the most common way to deal with different ancestries in a genotyped data set is to identify a genetically homogenous subset and perform the association analysis in the desired homogeneous subpopulation. There have been methods proposed that can account for ancestry such as tractor\cite{atkinson2021tractor}, but they have not been widely adopted yet. 

A homogenous subpopulation can be identified by performing a PCA on all the available individuals and calculating a robust Mahalanobis distance on the first, e.g.\ 20 PCs, and removing anyone above a certain threshold\cite{prive2020efficient}. 
\textbf{TODO: insert PC plot with ancestries colours to illustrate feasibility}

\subsubsection{Relatedness}
Similar to population stratification, relatedness is a common cause of spurious associations. The mechanism behind why relatedness leads to these spurious associations is a little different. If related individuals are in the same analysis, then some individuals are more alike than one would expect if they were drawn at random. Due to this, deviations from the null distribution are likely to occur, but not due to the effect of the SNP's effect, but rather the sampling. For a Wald test, this would lead to downwardly biased variance estimates, which leads to inflated test statistics, as the the test statistics is the effect estimate divided by the standard error. \textbf{REF?}

There are two common ways to deal with relatedness in a GWAS setting. The first and simplest way is to identify the related individuals and removing them from the analysis. This is effective, but has the downside of reducing the sample size. The second and more involved way is to include the in-sample relatedness (sometimes also called cryptic relatedness) in the model being used for association. In a linear regression setting, the most common way to account for the in-sample relatedness is by using a linear mixed model, where a random effect that models the genotype correlation is added. \textbf{TODO: Should I write more about the linear mixed models? such as a section under GWAS I can refer to here} The random effect is able to account for the covariance structure of the individuals, which is how relatedness affects associations with higher than expected observations\cite{yu2006unified, kang2008efficient}. The in-sample relatedness is accounted for by having the covariance structure of the random effect follow the GRM.

There are several ways to identify the related individuals, with the two most common ways being the genetic relatedness matrix(GRM) and identity by descent(IBD)\textbf{REF TO HOW THESE ARE DONE?}. The GRM consists of the correlation between individuals genotypes, where a value of $ 1 $ means monozygotic twins, $ 0.5 $ is a parent-offspring relationship, etc.. If filtering is performed prior to the association test, the relatedness threshold is usually set to $ 2^{-2.5} \approx 0.177 $ when removing $ 2^{nd} $ degree relatives or closer, or $ 2^{-3.5} \approx 0.088 $ when removing $ 3^{rd} $ degree relatives, etc. Filtering for relatedness with IBD is very similar to how it is done with the GRM, however the values are between $ 0 $ and $ 0.5 $ instead of $ 0 $ and $ 1 $ \textbf{TODO: are the values always between 0 and 0.5 for all IBD methods?}. To get the same level of relatedness filtering with IBD as one would get with the GRM, the thresholds should be shifted by a factor of $ 2^{-1} $ and will instead have thresholds $ 2^{-3.5} $ and $ 2^{-4.5} $, respectively. An IBD approach for identifying relatedness is provided by the KING software\cite{manichaikul2010robust}, and a GRM based approach is provided by the GCTA software \cite{yang2011gcta}. Both ways of estimating relatedness is also implemented in the PLINK software\cite{chang2015second,purcell2007plink}


\subsubsection{Multiple testing correction}
A GWAS consists of testing each available SNP for an association with the phenotype of interest. This means several million tests are often performed. A classical statical approach to hypothesis testing means a test has a significance threshold denoted by $ \alpha $, which is most commonly $ 5\% $. If the p-value is below $ \alpha $, the null hypothesis is rejected and the alternative hypothesis is accepted. Due to the p-values being uniformly distributed under the null hypothesis, we will expect to have $ (100\times \alpha) \%$ of the tests performed rejects the null hypothesis purely by chance. There are ways to account for this. The most common multiple testing correction method used in GWAS is the Bonferroni correction\textbf{REF}. As a motivation for the Bonferroni correction, let $ n $ independent tests be given, then the likelihood of seeing at least one false positive is given by

\begin{equation} \label{eq:bonferroni}
1 - (1-\alpha)^n 
\end{equation}
which leads to the Bonferroni correction $ \alpha_{bf} \approx \alpha/n $. By comparing the repeated tests against $ \alpha_{bf} $ the expected number of false positives will remain $ \alpha $ across all tests performed, instead of $ \alpha $ for each individual test. In a GWAS setting, it is common to assume 1 million independent tests are performed \textbf{REF?}, which leads to a genome-wide significance threshold of $ 5 \times 10^{-8} $.


\subsubsection{Saddle point approximation}
\textbf{TODO: introduce SPA, as it is used by several methods}
\textbf{TODO: i do not want to derive anything. mention it can be used to estimate the CDF given a moment generating function is known and that is it ?}



\subsection{Computational efficiency}
This section will cover some of the common computational and mathematical tricks used to speed up GWAS. 

There is a computational cost involved in estimating the effects of the covariates. Therefore, the most efficient way to account for the covariates without directly calculating their effect in each regression is to project them out of the predictor and the response of interest in \cref{eq:baseGWAS} \cite{sikorska2013gwas}. We will briefly go over how this projection is performed \textbf{TODO:add ref to where this is done}.

\subsubsection{Projecting covariates}
For the sake of completeness, we will present how to regress out the covariates as they were presented by Sikorska et al.\cite{sikorska2013gwas}. If we consider the residual sum of squares(RSS) for \cref{eq:baseGWAS}, we get 

\begin{align}
	RSS = \left( y - \beta G_j - X\gamma \right)^T\left( y - \beta G_j - X\gamma \right) = &y^T y - 2\beta y^T G_j - 2y^TX\gamma \\
	&- \beta^2 G_j^TG_j + 2\beta G_j^T X + \gamma^T X^T X \gamma.
\end{align}
Recall that $ X\gamma $ is a vector of dimension $ N \times 1 $, which means $ y_T X \gamma $ is an inner product and inner products are symmetric. Differentiating the residual sum of squares with respect to $ \beta $ and $ \gamma $ yields

\begin{align}
	\dfrac{\partial}{\partial \beta} (RSS) = -2y^TG + 2\beta G_j^TG_j + 2G_j^T X\gamma \\
	\dfrac{\partial}{\partial \gamma} (RSS)  = -2y^TX + 2\beta G^T_j X + 2X^TX\gamma 
\end{align}
Setting these expressions equal to $ 0 $, we get
\begin{align}
	G_j^T G_j \beta +  G_j^T X \gamma =  G_j^T y  \label{eq:covregress1}\\ 
	X^T G_j \beta + X^T X \gamma =  X^Ty \label{eq:covregress2}
\end{align}
This means the matrix notation of the least squares solution to \cref{eq:baseGWAS} is given by 
\begin{equation}
	\begin{pmatrix}
		G_j^T G_j & G_j^T X \\
		X^T G_j & X^T X
	\end{pmatrix}
	\begin{pmatrix}
		\hat{\beta} \\
		\hat{\gamma}
	\end{pmatrix} = 
	\begin{pmatrix}
		G_j^T y \\
		X^T y
	\end{pmatrix}.
\end{equation}
and we will let $ \hat{\beta} $ and $ \hat{\gamma} $ denote solutions to the least squares equations. However, we are interested in an expression that does not depend on the covariates. From here we isolate $ \hat{\gamma} $ in \cref{eq:covregress2} and get $ \hat{\gamma} = (X^TX)^{-1}(X^Ty - \hat{\beta} X^TG_j) $, which is then inserted in to \cref{eq:covregress1} 

\begin{equation}
	G^T_j y = G_j^TG_j + G_j^TX (X^TX)^{-1}(X^Ty - \hat{\beta} X^TG_j).
\end{equation}
By isolating terms related to $ y $ on the left hand side and term related to $ \hat{\beta} $ on the right hand side we get the following

\begin{equation} \label{eq:GWASprojection}
	G_j^T(y - X(X^TX)^{-1}X^Ty) = G_j^T(G_j - X(X^TX)^{-1}X^TG_j) \hat{\beta}.
\end{equation}
Recall that $ X(X^TX)^{-1}X^T $ denotes the projection onto the space spanned by the matrix $ X $. From here, we will introduce transformations given by 
\begin{align}
	y^\ast = y - X(X^TX)^{-1}X^Ty & & & G_j^{\ast} = G_j - X(X^TX)^{-1}X^TG_j.
\end{align}
The transformations remove the effect of the covariates in $ X $ from the response and predictor of interest. Using the properties of projections, \cref{eq:GWASprojection}, and the transformations, we find that 

\begin{equation}
	\left( G_j^{\ast} \right)^T G_j^{\ast} \hat{\beta} = G_j^T G_j^{\ast} \hat{\beta} \stackrel{\ref{eq:GWASprojection}}{=} G_j^T y^{\ast} = \left( G_j^{\ast} \right)^T y^{\ast}.
\end{equation}
The normal equation for systems of equations of the form $ Ax=b $ say that $ \hat{\beta} $ is a solution to a new univariate regression given by

\begin{align}\label{eq:univarGWAS}
	y^\ast = \hat{\beta} G_j^{\ast} + \varepsilon&   &\text{with simplified solution}&  &\hat{\beta} = \dfrac{\left( G_j^{\ast} \right)^T y^{\ast}}{\left( G_j^{\ast} \right)^T G^{\ast}}.
\end{align}
With the projection, the effect of the covariates have been removed from the outcome and the predictor, i.e. the phenotype and the genotype do \textit{not} depend on $ \gamma $ any more. Therefore, the calculations have been simplified and the calculations for the projection matrix only has to be performed once. Accounting for the covariate's effect in the phenotype also only has to be done once, the removal of the covariate's effect on the SNP has to be done for each SNP separately.
\textbf{TODO: Florian's thesis deals with speeding up computations even further, mention this too?}


\subsection{Power improvement}
\textbf{TODO: refine phenotype reduces residual var -> increases power}






\subsection{Notable methodological advancements}
This section provides a non-exhaustive list of methodological advances proposed for GWAS. The list aims to highlight key advances that have been made by either providing computational feasibility for a certain type of analysis, use of a more complex model, or both. Notable GWAS methods are presented in \cref{table:GWASoverview}. 

\begin{table}[h]
	\centering
	\begin{tabularx}{\textwidth}{l X l}
		\hline
		Software	&	Notable advancement		&	Model \\
		\hline
		PLINK\cite{chang2015second,purcell2007plink}	&
		Highly scalable linear and logistic regression \& Data management and standardized a binary storage format	&
		Linear \& logistic regression	\\
		BOLT\cite{loh2015efficient}	&
		Efficient linear mixed model for UKBB sized data that accounts for cryptic relatedness	&
		Linear mixed model	\\
		SPACox\cite{bi2020fast}	&	
		Saddle point approximation based proportional hazards model for UKBB sized data &
		Cox proportional hazards \\
		GATE\cite{dey2022efficient}	&
		Saddle point approximation based frailty model for UKBB sized data	&
		Frailty model \\
		\hline
	\end{tabularx}
	\caption{Overview of notable GWAS methods}
	\label{table:GWASoverview}
\end{table}
Other methods that are based on the saddle point approximation (SPA)\cite{daniels1954saddlepoint,kuonen1999miscellanea} have been proposed by methods such as SAIGE\cite{zhou2018efficiently} and REGENIE\cite{mbatchou2021computationally}. One of the advantages of using SPA is that it provides good control of Type 1 error, even for unbalanced case-control phenotypes. While BOLT-LMM provided an efficient implementation for linear mixed models, further study of the software have revealed that it suffers from inflated test statistics when case-control ratio is $ 1:50 $ or higher. SPA-based methods do not suffer from inflation in such cases\cite{mbatchou2021computationally}.

